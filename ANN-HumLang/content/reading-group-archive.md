---
title: "Reading group archive"
---

In the 2023 Spring Semester we organized a reading group at the ILLC, bringing together formal semanticists and computational linguists working at our institute to discuss the use of Artificial Neural Networks in modelling human language learning and processing. Below is an archive of the papers we read.

Feel free to keep suggesting papers in our [sheet](https://docs.google.com/spreadsheets/d/1h_HJATlutSkfDKYTtoR3zvnCFhUsmhfi1VF-QfodK7w/edit?usp=sharing)!

#### April 3rd, 2023
Li, J., Yu, L., & Ettinger, A. (2022). Counterfactual reasoning: Do language models need world knowledge for causal inference? In _NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI)_. [https://arxiv.org/pdf/2212.03278.pdf](https://arxiv.org/pdf/2212.03278.pdf)

_presented by:_ Tom  
_time:_ 11-12am CET  
_location:_ LAB42, L6.51

#### March 27th, 2023
Piantadosi, S. (2023). Modern language models refute Chomsky's approach to language. _LingBuzz_. [https://lingbuzz.net/lingbuzz/007180](https://lingbuzz.net/lingbuzz/007180)

_time:_ 4-5pm CET  
_location:_ LAB42, **L6.27**

#### March 20th, 2023
Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U., & Levy, O. (2020). Emergent linguistic structure in artificial neural networks trained by self-supervision. _Proceedings of the National Academy of Sciences, 117_(48), 30046-30054. [https://doi.org/10.1073/pnas.1907367117](https://doi.org/10.1073/pnas.1907367117)

_presented by:_ Samuel  
_time:_ 2-3pm CET  
_location:_ LAB42, L6.51

#### March 13th, 2023
Merrill, W., Warstadt, A., & Linzen, T. (2022). Entailment Semantics Can Be Extracted from an Ideal Language Model. In _Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)_, pages 176–193, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. [https://aclanthology.org/2022.conll-1.13/](https://aclanthology.org/2022.conll-1.13/)

_presented by:_ Taka  
_time:_ 4-5pm CET  
_location:_ LAB42, L6.51

#### March 6th, 2023
McClelland, J. L., Hill, F., Rudolph, M., Baldridge, J., & Schütze, H. (2020). Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. _Proceedings of the National Academy of Sciences, 117_(42), 25966-25974. [https://doi.org/10.1073/pnas.1910416117](https://doi.org/10.1073/pnas.1910416117)

_presented by:_ Marianne  
_time:_ 4-5pm CET  
_location:_ LAB42, L6.51

#### February 27th, 2023
Jain, S., Vo, V. A., Wehbe, L., & Huth, A. G. (2023). Computational language modeling and the promise of in silico experimentation. _Neurobiology of Language_, 1-65. [https://doi.org/10.1162/nol_a_00101](https://doi.org/10.1162/nol_a_00101)

_presented by:_ Tamar  
_time:_ 4-5pm CET  
_location:_ LAB42, L6.51

#### February 21st, 2023
Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022. [https://arxiv.org/abs/2204.02329](https://arxiv.org/abs/2204.02329)

_presented by:_ Taka  
_time:_ 4-5pm CET  
_location:_ LAB42, **L3.06**  

#### February 13th, 2023
Lakretz, Y., Desbordes, T., Hupkes, D., & Dehaene, S. (2022, October). Can Transformers Process Recursive Nested Constructions, Like Humans?. In _Proceedings of the 29th International Conference on Computational Linguistics_ (pp. 3226-3232). [https://aclanthology.org/2022.coling-1.285/](https://aclanthology.org/2022.coling-1.285/)

_presented by:_ Tom  
_time:_ 4-5pm CET  
_location:_ LAB42, L6.51

#### February 6th, 2023
Mahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., & Fedorenko, E. (2023). Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627. [https://arxiv.org/abs/2301.06627](https://arxiv.org/abs/2301.06627)

_presented by:_ Marianne  
_time:_ 4-5pm CET  
_location:_ LAB42, L6.51

#### January 30th, 2023
Warstadt, A., & Bowman, S. R. (2022). What artificial neural networks can tell us about human language acquisition. In _Algebraic Structures in Natural Language_ (pp. 17-60). CRC Press. [https://arxiv.org/abs/2208.07998](https://arxiv.org/abs/2208.07998)

_presented by:_ Tamar  
_time:_ 4-5pm CET  
_location:_ LAB42, L6.51