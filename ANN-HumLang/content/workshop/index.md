---
title: "Workshop"
---

### Workshop (10-12 June, 2024)
Artificial Neural Networks (ANNs) have proven to be powerful learning devices for language-related tasks, as demonstrated by recent progress in artificial intelligence driven by large, Transformer-based language models. But how can ANNs inform us about _human_ language learning and processing? Our three-day workshop brings together researchers working on cognitively motivated and linguistic questions in studying the language processing mechanisms and learning trajectories of ANNs. 

For the first two days of the programme, we hope to stimulate discussion on the workshop theme through contributed presentations from our workshop participants and keynote speakers. The final day will be focussed on active interactions and collaboration between participants, through small-scale tutorials and joint group work on a collaborative task.

We will share more information on this page soon, but please feel free to contact us if you are interested in attending!
<!-- We are currently accepting participants to our workshop on an invitation basis, focussing on early-career researchers working on topics relevant to the workshop theme. However, please feel free to contact us if you are interested in attending! (Especially if you are able to cover your own travel and accommodation fees) -->

#### Organizers
Tamar Johnson (t.johnson@uva.nl)  
Marianne de Heer Kloots (m.l.s.deheerkloots@uva.nl)

#### Venue
Institute for Logic, Language and Computation  
Amsterdam Science Park campus, University of Amsterdam  

#### Confirmed keynote speakers:

[Arianna Bisazza](https://www.cs.rug.nl/~bisazza/) (University of Groningen)  
{{< details "_Can modern Language Models be truly polyglot? Language learnability and inequalities in NLP_" >}}
Despite their impressive advances, modern Language Models (LMs) are still far from reaching language equality, i.e. comparable performance in all languages.
The uneven amount of data available in different languages is often recognized as the main culprit. However, another obstacle to language equality is posed by the observation that some languages are intrinsically more difficult to model than others by modern LM architectures, even when training data size is controlled for.

In this talk, I will present evidence supporting this observation, coming from different tasks and different evaluation methodologies (e.g. using natural versus synthetic languages).
I will then argue for the usefulness of artificial languages to unravel the complex interplay between language properties and learnability by neural networks.
Finally, I will provide an outlook of my upcoming project aimed at improving language modeling for (low-resource) morphologically rich languages, taking inspiration from child language acquisition.
{{< /details >}}

[Eva Portelance](https://evaportelance.github.io/) (Mila; HEC Montréal)  
{{< details "_What neural networks can teach us about how we learn language_" >}}
How can modern neural networks like large language models be useful to the field of language acquisition, and more broadly cognitive science, if they are not a priori designed to be cognitive models? As developments towards natural language understanding and generation have improved leaps and bounds, with models like GPT-4, the question of how they can inform our understanding of human language acquisition has re-emerged. This talk address how AI models as objects of study can indeed be useful tools for understanding how humans learn language. It will present three approaches for studying human learning behaviour using different types of neural networks and experimental designs, each illustrated through a specific case study. 

Understanding how humans learn is an important problem for cognitive science and a window into how our minds work. Additionally, human learning is in many ways the most efficient and effective algorithm there is for learning language; understanding how humans learn can help us design better AI models in the future.
{{< /details >}}

[Ethan Wilcox](https://wilcoxeg.github.io/) (ETH Zürich)  
{{< details "_Title TBA_" >}}
Abstract TBA
{{< /details >}}

#### Programme

We hope to publish our complete workshop programme within the next few weeks, check back soon!

##### Monday June 10th
{{< rawhtml >}}
<table>
<tbody>
  <tr>
    <td>13.00 - 18.00</td>
    <td>TBA</td>
    <td></td>
    <td></td>
  </tr>
</tbody>
</table>
{{< /rawhtml >}}

##### Tuesday June 11th
{{< rawhtml >}}
<table>
<tbody>
  <tr>
    <td>09.30 - 18.00</td>
    <td>TBA</td>
    <td></td>
    <td></td>
  </tr>
</tbody>
</table>
{{< /rawhtml >}}

##### Wednesday June 12th
{{< rawhtml >}}
<table>
<tbody>
  <tr>
    <td>09.30 - 18.00</td>
    <td>TBA</td>
    <td></td>
    <td></td>
  </tr>
</tbody>
</table>
{{< /rawhtml >}}

#### Acknowledgements
This workshop is supported by and organized as part of the _Language in Interaction_ consortium (NWO Gravitation Grant 024.001.006).

{{< rawhtml >}}
<a href="https://www.dcc.ru.nl/languageininteraction/"><img src="/images/LiI_Logo_450px.jpg" width=300></img></a>
{{< /rawhtml >}}