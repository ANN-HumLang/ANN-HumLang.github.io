<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Using Artificial Neural Networks for Studying Human Language Learning and Processing</title>
    <link>https://ANN-HumLang.github.io/</link>
    <description>Recent content on Using Artificial Neural Networks for Studying Human Language Learning and Processing</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <managingEditor>me@example.com (Tamar Johnson &amp; Marianne de Heer Kloots)</managingEditor>
    <webMaster>me@example.com (Tamar Johnson &amp; Marianne de Heer Kloots)</webMaster>
    <copyright>Tamar Johnson &amp; Marianne de Heer Kloots</copyright>
    <atom:link href="https://ANN-HumLang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reading group archive</title>
      <link>https://ANN-HumLang.github.io/reading-group-archive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>me@example.com (Tamar Johnson &amp; Marianne de Heer Kloots)</author>
      <guid>https://ANN-HumLang.github.io/reading-group-archive/</guid>
      <description>In the 2023 Spring Semester we organized a reading group at the ILLC, bringing together formal semanticists and computational linguists working at our institute to discuss the use of Artificial Neural Networks in modelling human language learning and processing. Below is an archive of the papers we read.&#xA;Feel free to keep suggesting papers in our sheet!&#xA;April 3rd, 2023 Li, J., Yu, L., &amp;amp; Ettinger, A. (2022). Counterfactual reasoning: Do language models need world knowledge for causal inference?</description>
      <content:encoded><![CDATA[<p>In the 2023 Spring Semester we organized a reading group at the ILLC, bringing together formal semanticists and computational linguists working at our institute to discuss the use of Artificial Neural Networks in modelling human language learning and processing. Below is an archive of the papers we read.</p>
<p>Feel free to keep suggesting papers in our <a href="https://docs.google.com/spreadsheets/d/1h_HJATlutSkfDKYTtoR3zvnCFhUsmhfi1VF-QfodK7w/edit?usp=sharing">sheet</a>!</p>
<h4 id="april-3rd-2023">April 3rd, 2023</h4>
<p>Li, J., Yu, L., &amp; Ettinger, A. (2022). Counterfactual reasoning: Do language models need world knowledge for causal inference? In <em>NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI)</em>. <a href="https://arxiv.org/pdf/2212.03278.pdf">https://arxiv.org/pdf/2212.03278.pdf</a></p>
<p><em>presented by:</em> Tom<br>
<em>time:</em> 11-12am CET<br>
<em>location:</em> LAB42, L6.51</p>
<h4 id="march-27th-2023">March 27th, 2023</h4>
<p>Piantadosi, S. (2023). Modern language models refute Chomsky&rsquo;s approach to language. <em>LingBuzz</em>. <a href="https://lingbuzz.net/lingbuzz/007180">https://lingbuzz.net/lingbuzz/007180</a></p>
<p><em>time:</em> 4-5pm CET<br>
<em>location:</em> LAB42, <strong>L6.27</strong></p>
<h4 id="march-20th-2023">March 20th, 2023</h4>
<p>Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U., &amp; Levy, O. (2020). Emergent linguistic structure in artificial neural networks trained by self-supervision. <em>Proceedings of the National Academy of Sciences, 117</em>(48), 30046-30054. <a href="https://doi.org/10.1073/pnas.1907367117">https://doi.org/10.1073/pnas.1907367117</a></p>
<p><em>presented by:</em> Samuel<br>
<em>time:</em> 2-3pm CET<br>
<em>location:</em> LAB42, L6.51</p>
<h4 id="march-13th-2023">March 13th, 2023</h4>
<p>Merrill, W., Warstadt, A., &amp; Linzen, T. (2022). Entailment Semantics Can Be Extracted from an Ideal Language Model. In <em>Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)</em>, pages 176–193, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. <a href="https://aclanthology.org/2022.conll-1.13/">https://aclanthology.org/2022.conll-1.13/</a></p>
<p><em>presented by:</em> Taka<br>
<em>time:</em> 4-5pm CET<br>
<em>location:</em> LAB42, L6.51</p>
<h4 id="march-6th-2023">March 6th, 2023</h4>
<p>McClelland, J. L., Hill, F., Rudolph, M., Baldridge, J., &amp; Schütze, H. (2020). Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. <em>Proceedings of the National Academy of Sciences, 117</em>(42), 25966-25974. <a href="https://doi.org/10.1073/pnas.1910416117">https://doi.org/10.1073/pnas.1910416117</a></p>
<p><em>presented by:</em> Marianne<br>
<em>time:</em> 4-5pm CET<br>
<em>location:</em> LAB42, L6.51</p>
<h4 id="february-27th-2023">February 27th, 2023</h4>
<p>Jain, S., Vo, V. A., Wehbe, L., &amp; Huth, A. G. (2023). Computational language modeling and the promise of in silico experimentation. <em>Neurobiology of Language</em>, 1-65. <a href="https://doi.org/10.1162/nol_a_00101">https://doi.org/10.1162/nol_a_00101</a></p>
<p><em>presented by:</em> Tamar<br>
<em>time:</em> 4-5pm CET<br>
<em>location:</em> LAB42, L6.51</p>
<h4 id="february-21st-2023">February 21st, 2023</h4>
<p>Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022. <a href="https://arxiv.org/abs/2204.02329">https://arxiv.org/abs/2204.02329</a></p>
<p><em>presented by:</em> Taka<br>
<em>time:</em> 4-5pm CET<br>
<em>location:</em> LAB42, <strong>L3.06</strong></p>
<h4 id="february-13th-2023">February 13th, 2023</h4>
<p>Lakretz, Y., Desbordes, T., Hupkes, D., &amp; Dehaene, S. (2022, October). Can Transformers Process Recursive Nested Constructions, Like Humans?. In <em>Proceedings of the 29th International Conference on Computational Linguistics</em> (pp. 3226-3232). <a href="https://aclanthology.org/2022.coling-1.285/">https://aclanthology.org/2022.coling-1.285/</a></p>
<p><em>presented by:</em> Tom<br>
<em>time:</em> 4-5pm CET<br>
<em>location:</em> LAB42, L6.51</p>
<h4 id="february-6th-2023">February 6th, 2023</h4>
<p>Mahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., &amp; Fedorenko, E. (2023). Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627. <a href="https://arxiv.org/abs/2301.06627">https://arxiv.org/abs/2301.06627</a></p>
<p><em>presented by:</em> Marianne<br>
<em>time:</em> 4-5pm CET<br>
<em>location:</em> LAB42, L6.51</p>
<h4 id="january-30th-2023">January 30th, 2023</h4>
<p>Warstadt, A., &amp; Bowman, S. R. (2022). What artificial neural networks can tell us about human language acquisition. In <em>Algebraic Structures in Natural Language</em> (pp. 17-60). CRC Press. <a href="https://arxiv.org/abs/2208.07998">https://arxiv.org/abs/2208.07998</a></p>
<p><em>presented by:</em> Tamar<br>
<em>time:</em> 4-5pm CET<br>
<em>location:</em> LAB42, L6.51</p>
]]></content:encoded>
    </item>
    <item>
      <title>Workshop</title>
      <link>https://ANN-HumLang.github.io/workshop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>me@example.com (Tamar Johnson &amp; Marianne de Heer Kloots)</author>
      <guid>https://ANN-HumLang.github.io/workshop/</guid>
      <description>Workshop (10-12 June, 2024) Artificial Neural Networks (ANNs) have proven to be powerful learning devices for language-related tasks, as demonstrated by recent progress in artificial intelligence driven by large, Transformer-based language models. But how can ANNs inform us about human language learning and processing? Our three-day workshop brings together researchers working on cognitively motivated and linguistic questions in studying the language processing mechanisms and learning trajectories of ANNs.&#xA;For the first two days of the programme, we hope to stimulate discussion on the workshop theme through contributed presentations from our workshop participants and keynote speakers.</description>
      <content:encoded><![CDATA[<h3 id="workshop-10-12-june-2024">Workshop (10-12 June, 2024)</h3>
<p>Artificial Neural Networks (ANNs) have proven to be powerful learning devices for language-related tasks, as demonstrated by recent progress in artificial intelligence driven by large, Transformer-based language models. But how can ANNs inform us about <em>human</em> language learning and processing? Our three-day workshop brings together researchers working on cognitively motivated and linguistic questions in studying the language processing mechanisms and learning trajectories of ANNs.</p>
<p>For the first two days of the programme, we hope to stimulate discussion on the workshop theme through contributed presentations from our workshop participants and keynote speakers. The final day is focussed on active interactions and collaboration between participants, through small-scale tutorials and joint group work on a collaborative task. See our provisional <a href="/workshop/#programme">programme</a> below for more information and currently confirmed participants!</p>
<h4 id="registration">Registration</h4>
<p>Registration is now open! Please 
<a href="https://forms.gle/6yn1ZRv7jf6pAVyE6"><button><b>register here</b></button></a> if you are interested in attending. Participation in the workshop is free of charge but the number of participants is limited. Priority will be given to junior researchers (PhD students and postdocs) and researchers who take part in the poster session and the collaborative task if we need to limit the number of attendees.</p>
<h4 id="organizers">Organizers</h4>
<p>Tamar Johnson (<a href="mailto:t.johnson@uva.nl">t.johnson@uva.nl</a>)<br>
Marianne de Heer Kloots (<a href="mailto:m.l.s.deheerkloots@uva.nl">m.l.s.deheerkloots@uva.nl</a>)</p>
<h4 id="venue">Venue</h4>
<p>Institute for Logic, Language and Computation<br>
SustainaLab event space, Amsterdam Science Park campus, University of Amsterdam</p>
<h4 id="confirmed-keynote-speakers">Confirmed keynote speakers:</h4>
<p><a href="https://www.cs.rug.nl/~bisazza/">Arianna Bisazza</a> (University of Groningen)<br>
<details>
    <summary><em>Can modern Language Models be truly polyglot? Language learnability and inequalities in NLP</em></summary>
    <p>Despite their impressive advances, modern Language Models (LMs) are still far from reaching language equality, i.e. comparable performance in all languages.
The uneven amount of data available in different languages is often recognized as the main culprit. However, another obstacle to language equality is posed by the observation that some languages are intrinsically more difficult to model than others by modern LM architectures, even when training data size is controlled for.</p>
<p>In this talk, I will present evidence supporting this observation, coming from different tasks and different evaluation methodologies (e.g. using natural versus synthetic languages).
I will then argue for the usefulness of artificial languages to unravel the complex interplay between language properties and learnability by neural networks.
Finally, I will provide an outlook of my upcoming project aimed at improving language modeling for (low-resource) morphologically rich languages, taking inspiration from child language acquisition.</p>

</details></p>
<p><a href="https://evaportelance.github.io/">Eva Portelance</a> (Mila; HEC Montréal)<br>
<details>
    <summary><em>What neural networks can teach us about how we learn language</em></summary>
    <p>How can modern neural networks like large language models be useful to the field of language acquisition, and more broadly cognitive science, if they are not a priori designed to be cognitive models? As developments towards natural language understanding and generation have improved leaps and bounds, with models like GPT-4, the question of how they can inform our understanding of human language acquisition has re-emerged. This talk address how AI models as objects of study can indeed be useful tools for understanding how humans learn language. It will present three approaches for studying human learning behaviour using different types of neural networks and experimental designs, each illustrated through a specific case study.</p>
<p>Understanding how humans learn is an important problem for cognitive science and a window into how our minds work. Additionally, human learning is in many ways the most efficient and effective algorithm there is for learning language; understanding how humans learn can help us design better AI models in the future.</p>

</details></p>
<p><a href="https://wilcoxeg.github.io/">Ethan Wilcox</a> (ETH Zürich)<br>
<details>
    <summary><em>Using artificial neural networks to study human language processing: Two case studies and a warning</em></summary>
    Neural network language models are pure prediction engines, they have no communicative intent, and they do not learn language through social interactions. Despite this, I argue that they can be used to study human language processing, in particular, to empirically evaluate theories that are based on probability distributions over words. In the first half of this talk, I discuss two case studies in this vein, focusing on psycholinguistic theories of incremental processing, as well as regressions, or backward saccades between words. In the second half of the talk, I take a step back and discuss the impact of scaling for the usefulness of ANNs in psycholinguistics research. Scaling is the trend toward producing ever-larger models, both in terms of parameter counts and in terms of the amount of data they are trained on. While largely beneficial to performance on downstream benchmarking tasks, scaling has several downsides for computational psycholinguistics. I will discuss the scientific and practical challenges presented by scaling for neural network modeling, as well as the benefits that would result from human-scale language modeling research.
</details></p>
<h4 id="programme">Programme</h4>
<p>Note that time slots in the schedule below are still preliminary and subject to change. We hope to publish our finalized programme within the next few weeks, check back soon!</p>
<h5 id="monday-june-10th">Monday June 10th</h5>


<table>
<tbody>
  <tr>
    <td style="width:20%">13.30 - 14.00</td>
    <td style="width:80%">Walk-in / registration</td>
  </tr>
  <tr>
    <td>14.00 - 14.15</td>
    <td>Opening</td>
  </tr>
  <tr>
    <td>14.15 - 15.15</td>
    <td><b>Keynote lecture</b> <br> Arianna Bisazza - <i>Can modern Language Models be truly polyglot? Language learnability and inequalities in NLP</i></td>
  </tr>
  <tr>
    <td>15.15 - 16.15</td>
    <td><b>Talks</b> <br> Confirmed speakers: <br><a href="https://www.universiteitleiden.nl/en/staffmembers/tessa-verhoef">Tessa Verhoef</a> (Leiden University), <br><a href="https://lgalke.github.io/">Lukas Galke</a> (MPI Nijmegen)</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>16:15 - 16:30</td>
    <td>Break</td>
  </tr>
  <tr>
    <td>16:30 - 18:00</td>
    <td><b>Poster session</b></td>
  </tr>
  <tr>
    <td>19:00 - 21:00</td>
    <td>Workshop dinner</td>
  </tr>
</tbody>
</table>

<h5 id="tuesday-june-11th">Tuesday June 11th</h5>


<table>
<tbody>
  <tr>
    <td style="width:20%">09.30 - 10.30</td>
    <td style="width:80%"><b>Keynote lecture</b><br>
    Eva Portelance - <i>What neural networks can teach us about how we learn language</i></td>
  </tr>
  <tr>
    <td>10.30 - 12.45</td>
    <td>Talk session and moderated discussion on <b>language learning</b> <br>Confirmed speakers: <br><a href="https://rgalhama.github.io/">Raquel Alhama</a> (University of Amsterdam), <br><a href="https://mahowak.github.io/">Kyle Mahowald</a> (University of Texas at Austin), <br><a href="https://yevgen.web.rug.nl/">Yevgen Matusevych</a> (University of Groningen)</td>
  </tr>
  <tr>
    <td>12.45 - 14.00</td>
    <td>Lunch</td>
  </tr>
  <tr>
    <td>14.00 - 15.00</td>
    <td><b>Keynote lecture</b><br>Ethan Wilcox - <i>Using artificial neural networks to study human language processing: Two case studies and a warning</i></td>
  </tr>
  <tr>
    <td>15.00 - 17.15</td>
    <td>Talk session and moderated discussion on <b>language processing</b><br>
    Confirmed speakers: <br><a href="https://scholar.google.nl/citations?user=hC9ygKAAAAAJ&hl=en&oi=ao">Pierre Orhan</a> (École Normale Supérieure), <br><a href="https://michaheilbron.github.io/">Micha Heilbron</a> (University of Amsterdam), <br><a href="http://www.stefanfrank.info/">Stefan Frank</a> (Radboud University Nijmegen)</td>
  </tr>
  <tr>
    <td>17.15 - 17.30</td>
    <td>Closing</td>
  </tr>
</tbody>
</table>

<h5 id="wednesday-june-12th">Wednesday June 12th</h5>


<table>
<tbody>
  <tr>
    <td style="width:20%">09.30 - 09.45</td>
    <td style="width:80%">Introduction to the tutorials and collaborative task</td>
  </tr>
  <tr>
    <td>09.45 - 10.30</td>
    <td>First tutorial by <a href="https://hconklin.com/">Henry Conklin</a> (University of Edinburgh): <br><i>Language learning as regularization - A non-parametric probing method for studying the emergence of structured representations over model training</i></td>
  </tr>
  <tr>
    <td>10.30 - 11.15</td>
    <td>Second tutorial by <a href="https://odvanderwal.nl/">Oskar van der Wal</a> & <a href="https://mdhk.net/">Marianne de Heer Kloots</a> (University of Amsterdam): <i>Training dynamics & Behavioural characterizations of grammar learning</i></td>
  </tr>
  <tr>
    <td>11.15 - 12.00</td>
    <td>Split up in groups & brainstorm</td>
  </tr>
  <tr>
    <td>12.00 - 13.00</td>
    <td>Lunch</td>
  </tr>
  <tr>
    <td>13.00 - 17.30</td>
    <td>Group work on collaborative task & Discussion of findings</td>
  </tr>
  <tr>
    <td>17.30</td>
    <td>Drinks</td>
  </tr>
</tbody>
</table>

<h4 id="acknowledgements">Acknowledgements</h4>
<p>This workshop is supported by and organized as part of the <em>Language in Interaction</em> consortium (NWO Gravitation Grant 024.001.006). We are also very thankful to the SustainaLab for lending us their space!</p>


<div class="flex-container">
    <div class="flex-item-left">
        <a href="https://www.dcc.ru.nl/languageininteraction/"><img src="/images/LiI_Logo_450px.jpg" width=300></img></a>
    </div>
    <div class="flex-item-center"></div>
    <div class="flex-item-right">
        <a href="https://sustainalab.nl/"><img src="https://www.sustainalab.nl/wp-content/uploads/2023/11/SustainaLab_Logo_RGB.png" width=150></img></a>
    </div>
</div>

]]></content:encoded>
    </item>
  </channel>
</rss>
